{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text, strip_punctuation, strip_short\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change DATA_DIR per your local filepath\n",
    "\n",
    "DATA_DIR = '/home/mandar/Data/NCSU/TropeAnalysis/TropesDataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre_movie_list(genre: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    This function returns all the json filenames containing movie dialogs\n",
    "    Args:\n",
    "        genre (str): String containing genre name.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of strings containing json filenames.\n",
    "    \"\"\"\n",
    "    movie_genre_json_list = []\n",
    "    if os.path.exists(join(DATA_DIR, 'ScreenPy', 'ParserOutput', genre)):\n",
    "        movies_per_genre = os.listdir(join(DATA_DIR, 'ScreenPy', 'ParserOutput', genre))\n",
    "        for movie in movies_per_genre:\n",
    "            if movie.endswith('.json'):\n",
    "                movie_genre_json_list.append(movie)\n",
    "    else:\n",
    "        print(\"Genre path does not exist!\")\n",
    "    return movie_genre_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_movie_dialog_file(genre: str, movie_filename: str) -> List[List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Loads the json data contained in movie file:\n",
    "    Args:\n",
    "        genre (str): String containing genre name.\n",
    "        movie_filename (str): String containing movie filename.\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Dict[str]]]: List of lists with each nested list containing a dictionary.\n",
    "    \"\"\"\n",
    "    with open(join(DATA_DIR, 'ScreenPy', 'ParserOutput', genre, movie_filename), 'r') as f:\n",
    "        movie_dialog_json = json.loads(f.read())\n",
    "    return movie_dialog_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_dialog_data(movie_json_data: List[List[Dict[str, str]]], \n",
    "                            verbose: bool = False):\n",
    "    \"\"\"\n",
    "    This function parses the movie json data, and collects the following information,\n",
    "        1. Unique characters with dialogs\n",
    "        2. Number of dialogs per character\n",
    "        3. Dialogs of all characters concatenated into a string\n",
    "    Args:\n",
    "        movie_json_data (List[List[Dict[str, str]]]): Json data containing movie character names and dialogs.\n",
    "        verbose (bool): Boolean indicating whether raw dialogs should be printed.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary with movie name as key and various nested dictionaries \n",
    "        containing data mentioned in function description.\n",
    "    \"\"\"\n",
    "    movie_characters = set()\n",
    "    movie_dialogs = list()\n",
    "    dialogs_per_character = defaultdict(int)\n",
    "    movie_info_dict = defaultdict()\n",
    "    for scene_dialogs in movie_json_data:\n",
    "        for dialog_info in scene_dialogs:\n",
    "            if 'speaker/title' in dialog_info['head_text']:\n",
    "                dialog_speaker = dialog_info['head_text']['speaker/title']\n",
    "                if verbose:\n",
    "                    print(f\"Speaker: {dialog_speaker}\")\n",
    "                    print(dialog_info['text'])\n",
    "                character = dialog_speaker.split('(')[0].strip()\n",
    "                movie_characters = movie_characters.union([character])\n",
    "                dialogs_per_character[character] += 1\n",
    "                movie_dialogs.append(dialog_info['text'])\n",
    "\n",
    "    movie_info_dict['characters'] = movie_characters\n",
    "    movie_info_dict['actor_dialog_count'] = dialogs_per_character\n",
    "    movie_info_dict['dialogs'] = ' '.join(movie_dialogs)\n",
    "    return movie_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 290 263\n",
      "Drama 579 399\n",
      "Thriller 373 242\n",
      "Comedy 347 199\n",
      "Crime 201 122\n",
      "Romance 192 102\n",
      "Adventure 166 72\n",
      "Sci-Fi 155 90\n",
      "Horror 149 83\n",
      "Animation 35 3\n",
      "War 26 14\n",
      "Family 39 13\n",
      "Musical 22 11\n",
      "Mystery 107 60\n"
     ]
    }
   ],
   "source": [
    "# Read all movie script json files for Action genre\n",
    "genres = ['Action', 'Drama', 'Thriller', 'Comedy', 'Crime', 'Romance', 'Adventure', 'Sci-Fi', 'Horror', \n",
    "          'Animation', 'War', 'Family', 'Musical', 'Mystery']\n",
    "genre_movie_json_list = []\n",
    "movie_names = []\n",
    "tropes = []\n",
    "genre_list = []\n",
    "for genre in genres:\n",
    "    genre_movie_json_list = get_genre_movie_list(genre)\n",
    "    # Remove .json file extension from movie filenames\n",
    "    movie_list = [movie.split('.json')[0] for movie in genre_movie_json_list]\n",
    "    # Find movies that match with TvTropes\n",
    "    # Read csv file mapping movie names containing script data with their tropes\n",
    "    genre_movie_script_trope_df = pd.read_csv(join(DATA_DIR, f'{genre.lower()}_movie_script_trope_match.csv'))\n",
    "    movie_match_df = genre_movie_script_trope_df.loc[genre_movie_script_trope_df.Movie_Script.isin(movie_list)].copy()\n",
    "    print(genre, len(movie_list), len(movie_match_df))\n",
    "    movie_names += movie_match_df.Movie_Script.tolist()\n",
    "    tropes += movie_match_df.Movie_Trope.tolist()    \n",
    "    genre_list += [genre] * len(movie_match_df)\n",
    "\n",
    "movie_tropes_df = pd.DataFrame(list(zip(movie_names, tropes, genre_list)), columns=['Movies', 'Tropes', 'Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_tropes_df = movie_tropes_df.drop_duplicates(subset=['Movies', 'Tropes'])\n",
    "movie_tropes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read TvTropes Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json file contianing movie tropes\n",
    "with open(join(DATA_DIR, 'films_tropes_20190501.json'), 'rb') as file:\n",
    "    tvtropes_json_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and pre-process movie script text,\n",
    "\n",
    "1. Parse json files with movie scripts and store them in a dict with movie name as key and script as value\n",
    "2. Additionally, preprocess the movie script text by converting them to lowercase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_movie_dialogs = defaultdict()\n",
    "all_preprocess_movie_dialogs = defaultdict()\n",
    "movie_trope_dict = defaultdict()\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), remove_stopwords, strip_punctuation, strip_short]\n",
    "\n",
    "for movie_row in movie_tropes_df.iterrows():\n",
    "    movie = movie_row[1].Movies\n",
    "    genre = movie_row[1].Genre\n",
    "    movie_filename = movie + '.json'\n",
    "    movie_json_data = load_json_movie_dialog_file(genre, movie_filename)\n",
    "    # Parse movie dialogs and preprocess text\n",
    "    all_raw_movie_dialogs[movie] = parse_movie_dialog_data(movie_json_data)\n",
    "    all_preprocess_movie_dialogs[movie] = preprocess_string(all_raw_movie_dialogs[movie]['dialogs'], CUSTOM_FILTERS)\n",
    "\n",
    "    # Collect list of tropes for the movie\n",
    "    movie_trope_dict[movie] = tvtropes_json_dict[movie_row[1].Tropes]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775, 775)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_trope_dict), len(all_preprocess_movie_dialogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all unique tropes per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tropes_set = list()\n",
    "for tropes in movie_trope_dict.values():\n",
    "    unique_tropes_set += list(set(tropes))\n",
    "    \n",
    "# Get movie count per trope\n",
    "tropes_count_dict = Counter(unique_tropes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tropes which appear in at least min_trope_count movies\n",
    "min_movie_per_trope_count = 1\n",
    "tropes_subset_list = list()\n",
    "for trope, count in tropes_count_dict.items():\n",
    "    if count >= min_movie_per_trope_count:\n",
    "        tropes_subset_list.append(trope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tropes present in at least 1 movies: 15192\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of tropes present in at least {min_movie_per_trope_count} movies: {len(tropes_subset_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF-IDF vectors and train xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_classifiers(X_train, X_test, y_train, y_test, classifier = 'xgb', \n",
    "                           multi_output = False, n_estimators = 100) -> np.array:\n",
    "    if classifier == 'xgb':\n",
    "        _fit = XGBClassifier(eval_metric='logloss', use_label_encoder=False, n_estimators=n_estimators)\n",
    "    elif classifier == 'rf':\n",
    "        _fit = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    if multi_output:\n",
    "        classifier = MultiOutputClassifier(estimator=_fit)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_hat = mutli_out_classifier.predict(X_test)\n",
    "        auc_class = np.zeros(len(y_test[0]))\n",
    "        for i in range(len(y_test[0])):\n",
    "            auc_class[i] = roc_auc_score(y_test[:, i],y_hat[:, i])\n",
    "    else:\n",
    "        _fit.fit(X_train, y_train)\n",
    "        y_hat = _fit.predict_proba(X_test)\n",
    "        auc_class = roc_auc_score(y_test,y_hat[:, 1])\n",
    "\n",
    "    return auc_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove movies with trope count less than threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avengersthe2012 has no tropes in json file\n",
      "autumninnewyork has no tropes in json file\n",
      "bigbluethe has no tropes in json file\n"
     ]
    }
   ],
   "source": [
    "# For each movie filter out tropes which appear in less than min_trope_count movies\n",
    "movie_tropes_subset_dict = defaultdict()\n",
    "for movie, trope in movie_trope_dict.items():\n",
    "    movie_tropes_subset_dict[movie] = list(set(tropes_subset_list).intersection(set(trope)))\n",
    "    if len(movie_tropes_subset_dict[movie]) == 0:\n",
    "        print(f'{movie} has no tropes in json file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775, 775)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_tropes_subset_dict), len(all_preprocess_movie_dialogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split new set of documents into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ShoutOut', 'OhCrap', 'ChekhovsGun', 'Foreshadowing', 'BittersweetEnding'],\n",
       " [345, 279, 259, 244, 217])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort tropes by their frequency count\n",
    "tropes_sorted_by_movie_count = {k: v for k, v in sorted(tropes_count_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "list(tropes_sorted_by_movie_count.keys())[:5], list(tropes_sorted_by_movie_count.values())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 620, test: 155\n",
      "BittersweetEnding: train ([446 174]), test ([112  43])\n"
     ]
    }
   ],
   "source": [
    "xgb_tfidf = []\n",
    "rf_tfidf = []\n",
    "max_features = [10, 50, 100, 500]\n",
    "\n",
    "rf_estimators = 200\n",
    "xgb_estimators = 200\n",
    "multi_output = False\n",
    "\n",
    "trope_of_interest = \"BittersweetEnding\"\n",
    "\n",
    "multi_label = MultiLabelBinarizer()\n",
    "y = multi_label.fit_transform([set(movie_tropes) for movie_tropes in movie_tropes_subset_dict.values()])\n",
    "trope_idx = trope_idx = list(multi_label.classes_).index(trope_of_interest)\n",
    "\n",
    "X_train_docs, X_test_docs, y_train, y_test = train_test_split(list(all_preprocess_movie_dialogs.values()), \n",
    "                                                              y[:, trope_idx], train_size=0.8, \n",
    "                                                              stratify=y[:, trope_idx])\n",
    "\n",
    "print(f'Train: {len(X_train_docs)}, test: {len(X_test_docs)}')\n",
    "print(f\"{trope_of_interest}: train ({np.bincount(y_train)}), test ({np.bincount(y_test)})\")\n",
    "\n",
    "X_train_tfidf_docs = [' '.join(list(x)) for x in X_train_docs]\n",
    "X_test_tfidf_docs = [' '.join(list(x)) for x in X_test_docs]\n",
    "\n",
    "for feature in max_features:\n",
    "    tfidf_fit = TfidfVectorizer(max_features=feature, ngram_range=(1, 2), stop_words='english').fit(raw_documents=X_train_tfidf_docs)\n",
    "    X_tfidf_train_vec = tfidf_fit.transform(X_train_tfidf_docs)\n",
    "    X_tfidf_test_vec = tfidf_fit.transform(X_test_tfidf_docs)\n",
    "\n",
    "    rf_auc_class = train_eval_classifiers(X_tfidf_train_vec, X_tfidf_test_vec, y_train, y_test, classifier='rf', \n",
    "                                          n_estimators = rf_estimators)\n",
    "    xgb_auc_class = train_eval_classifiers(X_tfidf_train_vec, X_tfidf_test_vec, y_train, y_test, \n",
    "                                           n_estimators = xgb_estimators)\n",
    "\n",
    "    if multi_output:\n",
    "        print(f\"RF Mean AUC: {round(np.mean(rf_auc_class), 2)}, RF Median AUC: {round(np.median(rf_auc_class), 2)}, RF Min AUC: {round(np.min(rf_auc_class), 2)}, RF Max AUC: {round(np.max(rf_auc_class), 2)}\")\n",
    "        print(f\"XGB Mean AUC: {round(np.mean(xgb_auc_class), 2)}, XGB Median AUC: {round(np.median(xgb_auc_class), 2)}, XGB Min AUC: {round(np.min(xgb_auc_class), 2)}, XGB Max AUC: {round(np.max(xgb_auc_class), 2)}\")\n",
    "    else:\n",
    "        rf_tfidf.append(rf_auc_class)\n",
    "        xgb_tfidf.append(xgb_auc_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Doc2Vec model on Movie Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert documents to TaggedDocument to train doc2vec models\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_preprocess_movie_dialogs.values())]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train doc2vec vectors and train xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vector_size = [10, 50, 100, 200]\n",
    "\n",
    "xgb_d2v = []\n",
    "rf_d2v = []\n",
    "\n",
    "trope_of_interest = \"BittersweetEnding\"\n",
    "\n",
    "multi_label = MultiLabelBinarizer()\n",
    "y = multi_label.fit_transform([set(movie_tropes) for movie_tropes in movie_tropes_subset_dict.values()])\n",
    "trope_idx = trope_idx = list(multi_label.classes_).index(trope_of_interest)\n",
    "\n",
    "X_train_docs, X_test_docs, y_train, y_test = train_test_split(documents, y[:, trope_idx], train_size=0.8, \n",
    "                                                              stratify=y[:, trope_idx])\n",
    "\n",
    "for v in vector_size:\n",
    "    dm_model = Doc2Vec(X_train_docs, vector_size=v, min_count=5, dm=1)\n",
    "    dbow_model = Doc2Vec(X_train_docs, vector_size=v, min_count=5, dm=0)\n",
    "\n",
    "    X_train_dm_dv = []\n",
    "    X_train_dbow_dv = []\n",
    "\n",
    "    X_test_dm_dv = []\n",
    "    X_test_dbow_dv = []\n",
    "\n",
    "    for i in range(len(X_train_docs)):\n",
    "        X_train_dm_dv.append(dm_model.docvecs[i])\n",
    "        X_train_dbow_dv.append(dbow_model.docvecs[i])\n",
    "\n",
    "    for i in range(len(X_test_docs)):\n",
    "        X_test_dm_dv.append(dm_model.infer_vector(X_test_docs[i][0]))\n",
    "        X_test_dbow_dv.append(dbow_model.infer_vector(X_test_docs[i][0]))\n",
    "\n",
    "    X_train_dv = pd.concat([pd.DataFrame(X_train_dm_dv), pd.DataFrame(X_train_dbow_dv)], axis=1)\n",
    "    X_test_dv = pd.concat([pd.DataFrame(X_test_dm_dv), pd.DataFrame(X_test_dbow_dv)], axis=1)\n",
    "\n",
    "    X_train_dv.columns = np.arange(X_train_dv.shape[1])\n",
    "    X_test_dv.columns = np.arange(X_test_dv.shape[1])\n",
    "\n",
    "    xgb_auc_class = train_eval_classifiers(X_train_dv, X_test_dv, y_train, y_test)\n",
    "    rf_auc_class = train_eval_classifiers(X_train_dv, X_test_dv, y_train, y_test, classifier='rf')\n",
    "\n",
    "    xgb_d2v.append(xgb_auc_class)\n",
    "    rf_d2v.append(rf_auc_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_scores(rf_tfidf, xgb_tfidf, rf_d2v, xgb_d2v, tfidf_vocab, d2v_vocab):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(np.arange(len(rf_tfidf)), rf_tfidf, color='red', marker='+', label='Random Forest')\n",
    "    ax1.plot(np.arange(len(xgb_tfidf)), xgb_tfidf, color='black', marker='o', label='XGBoost')\n",
    "    ax1.set_xlabel('Tf-Idf Vocabulary Size')\n",
    "    ax1.set_ylabel('AUC Score')\n",
    "    ax1.set_xticks(np.arange(len(tfidf_vocab)))\n",
    "    ax1.set_xticklabels(tfidf_vocab, rotation=45)\n",
    "    ax1.set_title(f\"AUC Scores Tf-Idf\")\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    ax2.plot(np.arange(len(rf_d2v)), rf_d2v, color='red', marker='+', label='Random Forest')\n",
    "    ax2.plot(np.arange(len(xgb_d2v)), xgb_d2v, color='black', marker='o', label='XGBoost')\n",
    "    ax2.set_xlabel('Doc2Vec Vocabulary Size')\n",
    "    ax2.set_ylabel('AUC Score')\n",
    "    ax2.set_xticks(np.arange(len(d2v_vocab)))\n",
    "    ax2.set_xticklabels(d2v_vocab, rotation=45)\n",
    "    ax2.set_title(f\"AUC Scores Doc2Vec\")\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_ylim([0, 1])    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_scores(rf_tfidf, xgb_tfidf, rf_d2v, xgb_d2v, max_features, vector_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
