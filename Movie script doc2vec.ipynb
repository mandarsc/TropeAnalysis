{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change DATA_DIR per your local filepath\n",
    "\n",
    "DATA_DIR = '/home/mandar/Data/NCSU/TropeAnalysis/TropesDataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre_movie_list(genre: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    This function returns all the json filenames containing movie dialogs\n",
    "    Args:\n",
    "        genre (str): String containing genre name.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of strings containing json filenames.\n",
    "    \"\"\"\n",
    "    movie_genre_json_list = []\n",
    "    movies_per_genre = os.listdir(join(DATA_DIR, 'ScreenPy', 'ParserOutput', genre))\n",
    "    for movie in movies_per_genre:\n",
    "        if movie.endswith('.json'):\n",
    "            movie_genre_json_list.append(movie)\n",
    "    return movie_genre_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_movie_dialog_file(genre: str, movie_filename: str) -> List[List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Loads the json data contained in movie file:\n",
    "    Args:\n",
    "        genre (str): String containing genre name.\n",
    "        movie_filename (str): String containing movie filename.\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Dict[str]]]: List of lists with each nested list containing a dictionary.\n",
    "    \"\"\"\n",
    "    with open(join(DATA_DIR, 'ScreenPy', 'ParserOutput', genre, movie_filename), 'r') as f:\n",
    "        movie_dialog_json = json.loads(f.read())\n",
    "    return movie_dialog_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_movie_dialog_data(movie_json_data: List[List[Dict[str, str]]], \n",
    "                            verbose: bool = False):\n",
    "    \"\"\"\n",
    "    This function parses the movie json data, and collects the following information,\n",
    "        1. Unique characters with dialogs\n",
    "        2. Number of dialogs per character\n",
    "        3. Dialogs of all characters concatenated into a string\n",
    "    Args:\n",
    "        movie_json_data (List[List[Dict[str, str]]]): Json data containing movie character names and dialogs.\n",
    "        verbose (bool): Boolean indicating whether raw dialogs should be printed.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary with movie name as key and various nested dictionaries \n",
    "        containing data mentioned in function description.\n",
    "    \"\"\"\n",
    "    movie_characters = set()\n",
    "    movie_dialogs = list()\n",
    "    dialogs_per_character = defaultdict(int)\n",
    "    movie_info_dict = defaultdict()\n",
    "    for scene_dialogs in movie_json_data:\n",
    "        for dialog_info in scene_dialogs:\n",
    "            if 'speaker/title' in dialog_info['head_text']:\n",
    "                dialog_speaker = dialog_info['head_text']['speaker/title']\n",
    "                if verbose:\n",
    "                    print(f\"Speaker: {dialog_speaker}\")\n",
    "                    print(dialog_info['text'])\n",
    "                character = dialog_speaker.split('(')[0].strip()\n",
    "                movie_characters = movie_characters.union([character])\n",
    "                dialogs_per_character[character] += 1\n",
    "                movie_dialogs.append(dialog_info['text'])\n",
    "\n",
    "    movie_info_dict['characters'] = movie_characters\n",
    "    movie_info_dict['actor_dialog_count'] = dialogs_per_character\n",
    "    movie_info_dict['dialogs'] = ' '.join(movie_dialogs)\n",
    "    return movie_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all movie script json files for Action genre\n",
    "genre = 'Action'\n",
    "genre_movie_json_list = get_genre_movie_list(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avatar.json',\n",
       " 'dune.json',\n",
       " 'blade.json',\n",
       " 'machete.json',\n",
       " 'startrekfirstcontact.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_movie_json_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json file contianing movie tropes\n",
    "with open(join(DATA_DIR, 'films_tropes_20190501.json'), 'rb') as file:\n",
    "    tvtropes_json_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .json file extension from movie filenames\n",
    "movie_list = [movie.split('.json')[0] for movie in genre_movie_json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file mapping movie names containing script data with their tropes\n",
    "action_movie_script_trope_df = pd.read_csv(join(DATA_DIR, 'action_movie_script_trope_match.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find movies that have tropes\n",
    "movie_match_df = action_movie_script_trope_df.loc[action_movie_script_trope_df.Movie_Script.isin(movie_list)].copy()\n",
    "len(movie_match_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse json files with movie scripts and store them in a dict with movie name as key and script as value\n",
    "# Additionally, preprocess the movie script text by converting them to lowercase \n",
    "all_raw_movie_dialogs = defaultdict()\n",
    "all_preprocess_movie_dialogs = defaultdict()\n",
    "movie_trope_dict = defaultdict()\n",
    "\n",
    "for movie_row in movie_match_df.iterrows():\n",
    "    movie = movie_row[1].Movie_Script\n",
    "    movie_filename = movie + '.json'\n",
    "    movie_json_data = load_json_movie_dialog_file(genre, movie_filename)\n",
    "    # Parse movie dialogs and preprocess text\n",
    "    all_raw_movie_dialogs[movie] = parse_movie_dialog_data(movie_json_data)\n",
    "    all_preprocess_movie_dialogs[movie] = simple_preprocess(all_raw_movie_dialogs[movie]['dialogs'])\n",
    "    \n",
    "    # Collect list of tropes for the movie\n",
    "    movie_trope_dict[movie] = tvtropes_json_dict[movie_row[1].Movie_trope]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 263)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_trope_dict), len(all_preprocess_movie_dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all unique tropes per movie\n",
    "unique_tropes_set = list()\n",
    "for tropes in movie_trope_dict.values():\n",
    "    unique_tropes_set += list(set(tropes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movie count per trope\n",
    "tropes_count_dict = Counter(unique_tropes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tropes which appear in at least min_trope_count movies\n",
    "threshold = 0.5\n",
    "min_movie_per_trope_count = 100\n",
    "tropes_subset_list = list()\n",
    "for trope, count in tropes_count_dict.items():\n",
    "    if count > min_movie_per_trope_count:\n",
    "        tropes_subset_list.append(trope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tropes present in at least 100 movies: 5\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of tropes present in at least {min_movie_per_trope_count} movies: {len(tropes_subset_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landofthedead has no tropes in json file\n",
      "tristanandisolde has no tropes in json file\n",
      "programthe has no tropes in json file\n",
      "next has no tropes in json file\n",
      "getawaythe has no tropes in json file\n",
      "newyorkminute has no tropes in json file\n",
      "entrapment has no tropes in json file\n",
      "warrior has no tropes in json file\n",
      "crouchingtigerhiddendragon has no tropes in json file\n",
      "supergirl has no tropes in json file\n",
      "cradle2thegrave has no tropes in json file\n",
      "mariachiel has no tropes in json file\n",
      "deepcover has no tropes in json file\n",
      "surrogates has no tropes in json file\n",
      "hostage has no tropes in json file\n",
      "stuntmanthe has no tropes in json file\n",
      "ticker has no tropes in json file\n",
      "avengersthe2012 has no tropes in json file\n",
      "bountyhunterthe has no tropes in json file\n",
      "battlelosangeles has no tropes in json file\n",
      "defiance has no tropes in json file\n",
      "hardtokill has no tropes in json file\n",
      "kingkong has no tropes in json file\n",
      "dune has no tropes in json file\n",
      "rambofirstbloodiithemission has no tropes in json file\n",
      "g.i.jane has no tropes in json file\n",
      "debtthe has no tropes in json file\n",
      "aloneinthedark has no tropes in json file\n",
      "siegethe has no tropes in json file\n",
      "timemachinethe has no tropes in json file\n"
     ]
    }
   ],
   "source": [
    "# For each movie filter out tropes which appear in less than min_trope_count movies\n",
    "movie_tropes_subset_dict = defaultdict()\n",
    "for movie, trope in movie_trope_dict.items():\n",
    "    movie_tropes_subset_dict[movie] = list(set(tropes_subset_list).intersection(set(trope)))\n",
    "    if len(movie_tropes_subset_dict[movie]) == 0:\n",
    "        print(f'{movie} has no tropes in json file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Doc2Vec model on Movie Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert documents to TaggedDocument to train doc2vec models\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_preprocess_movie_dialogs.values())]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_output = MultiLabelBinarizer()\n",
    "y = multi_output.fit_transform(list(movie_tropes_subset_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   8,   9,  10,  33,  38,  47,  49,  53,  75,  78,  81,  82,\n",
       "       101, 109, 123, 146, 149, 150, 155, 165, 176, 191, 200, 204, 231,\n",
       "       240, 244, 251, 258])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y.sum(axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove documents with no tropes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies left: 233\n"
     ]
    }
   ],
   "source": [
    "new_documents = []\n",
    "documents_with_no_tropes = np.where(y.sum(axis=1)==0)[0]\n",
    "for idx, document in enumerate(documents):\n",
    "    if idx not in documents_with_no_tropes:\n",
    "        new_documents.append(document)\n",
    "        \n",
    "y = y[y.sum(axis=1) > 0]\n",
    "print(f'Number of movies left: {len(new_documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split new set of documents into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs, X_test_docs, y_train, y_test = train_test_split(new_documents, y, \n",
    "                                                              train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 59, (174, 5), (59, 5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_docs), len(X_test_docs), y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF-IDF vectors and train xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_classifiers(X_train, X_test, y_train, y_test, classifier = 'xgb') -> np.array:\n",
    "    if classifier == 'xgb':\n",
    "        _fit = XGBClassifier(eval_metric='logloss', n_estimators=100)\n",
    "    elif classifier == 'rf':\n",
    "        _fit = RandomForestClassifier(n_estimators=50)\n",
    "    mutli_out_classifier = MultiOutputClassifier(estimator=_fit)\n",
    "    mutli_out_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_hat = mutli_out_classifier.predict(X_test)\n",
    "\n",
    "    auc_class = np.zeros(len(y_test[0]))\n",
    "\n",
    "    for i in range(len(y_test[0])):\n",
    "        auc_class[i] = roc_auc_score(y_test[:, i],y_hat[:, i])\n",
    "\n",
    "    return auc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train_docs = [' '.join(x[0]) for x in X_train_docs]\n",
    "X_tfidf_test_docs = [' '.join(x[0]) for x in X_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 10) (59, 10) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 10\n",
      "Mean AUC: 0.53, Median AUC: 0.53, Min AUC: 0.43, Max AUC: 0.65\n",
      "XGB with vocabulary size: 10\n",
      "Mean AUC: 0.54, Median AUC: 0.53, Min AUC: 0.44, Max AUC: 0.63\n",
      "(174, 50) (59, 50) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 50\n",
      "Mean AUC: 0.53, Median AUC: 0.53, Min AUC: 0.49, Max AUC: 0.59\n",
      "XGB with vocabulary size: 50\n",
      "Mean AUC: 0.53, Median AUC: 0.55, Min AUC: 0.47, Max AUC: 0.58\n",
      "(174, 100) (59, 100) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 100\n",
      "Mean AUC: 0.52, Median AUC: 0.51, Min AUC: 0.5, Max AUC: 0.58\n",
      "XGB with vocabulary size: 100\n",
      "Mean AUC: 0.52, Median AUC: 0.53, Min AUC: 0.47, Max AUC: 0.57\n",
      "(174, 500) (59, 500) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 500\n",
      "Mean AUC: 0.52, Median AUC: 0.53, Min AUC: 0.45, Max AUC: 0.57\n",
      "XGB with vocabulary size: 500\n",
      "Mean AUC: 0.55, Median AUC: 0.55, Min AUC: 0.5, Max AUC: 0.6\n",
      "(174, 1000) (59, 1000) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 1000\n",
      "Mean AUC: 0.52, Median AUC: 0.52, Min AUC: 0.45, Max AUC: 0.62\n",
      "XGB with vocabulary size: 1000\n",
      "Mean AUC: 0.55, Median AUC: 0.55, Min AUC: 0.52, Max AUC: 0.57\n",
      "(174, 1500) (59, 1500) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 1500\n",
      "Mean AUC: 0.54, Median AUC: 0.56, Min AUC: 0.47, Max AUC: 0.57\n",
      "XGB with vocabulary size: 1500\n",
      "Mean AUC: 0.55, Median AUC: 0.51, Min AUC: 0.45, Max AUC: 0.73\n",
      "(174, 2000) (59, 2000) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 2000\n",
      "Mean AUC: 0.52, Median AUC: 0.5, Min AUC: 0.47, Max AUC: 0.63\n",
      "XGB with vocabulary size: 2000\n",
      "Mean AUC: 0.57, Median AUC: 0.55, Min AUC: 0.5, Max AUC: 0.69\n",
      "(174, 5000) (59, 5000) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 5000\n",
      "Mean AUC: 0.5, Median AUC: 0.47, Min AUC: 0.43, Max AUC: 0.56\n",
      "XGB with vocabulary size: 5000\n",
      "Mean AUC: 0.53, Median AUC: 0.53, Min AUC: 0.44, Max AUC: 0.62\n",
      "(174, 10000) (59, 10000) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 10000\n",
      "Mean AUC: 0.51, Median AUC: 0.5, Min AUC: 0.49, Max AUC: 0.55\n",
      "XGB with vocabulary size: 10000\n",
      "Mean AUC: 0.56, Median AUC: 0.55, Min AUC: 0.52, Max AUC: 0.59\n",
      "(174, 20000) (59, 20000) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 20000\n",
      "Mean AUC: 0.52, Median AUC: 0.53, Min AUC: 0.48, Max AUC: 0.57\n",
      "XGB with vocabulary size: 20000\n",
      "Mean AUC: 0.57, Median AUC: 0.56, Min AUC: 0.54, Max AUC: 0.62\n",
      "(174, 25000) (59, 25000) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandar/.local/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with vocabulary size: 25000\n",
      "Mean AUC: 0.54, Median AUC: 0.57, Min AUC: 0.48, Max AUC: 0.58\n",
      "XGB with vocabulary size: 25000\n",
      "Mean AUC: 0.58, Median AUC: 0.58, Min AUC: 0.54, Max AUC: 0.61\n"
     ]
    }
   ],
   "source": [
    "max_features = [10, 50, 100, 500, 1000, 1500, 2000, 5000, 10000, 20000, 25000]\n",
    "for feature in max_features:\n",
    "    tfidf_fit = TfidfVectorizer(max_features=feature, stop_words='english').fit(raw_documents=X_tfidf_train_docs)\n",
    "    X_tfidf_train_vec = tfidf_fit.transform(X_tfidf_train_docs)\n",
    "    X_tfidf_test_vec = tfidf_fit.transform(X_tfidf_test_docs)\n",
    "    print(X_tfidf_train_vec.shape, X_tfidf_test_vec.shape, len(y_train[0]))\n",
    "    rf_auc_class = train_eval_classifiers(X_tfidf_train_vec, X_tfidf_test_vec, y_train, y_test, classifier='rf')\n",
    "    xgb_auc_class = train_eval_classifiers(X_tfidf_train_vec, X_tfidf_test_vec, y_train, y_test)\n",
    "    print(f'Random Forest with vocabulary size: {feature}')\n",
    "    print(f\"Mean AUC: {round(np.mean(rf_auc_class), 2)}, Median AUC: {round(np.median(rf_auc_class), 2)}, Min AUC: {round(np.min(rf_auc_class), 2)}, Max AUC: {round(np.max(rf_auc_class), 2)}\")\n",
    "    print(f'XGB with vocabulary size: {feature}')\n",
    "    print(f\"Mean AUC: {round(np.mean(xgb_auc_class), 2)}, Median AUC: {round(np.median(xgb_auc_class), 2)}, Min AUC: {round(np.min(xgb_auc_class), 2)}, Max AUC: {round(np.max(xgb_auc_class), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# xgb_auc_class = train_eval_classifiers(X_tfidf_train_vec, X_tfidf_test_vec, y_train, \n",
    "#                                        y_test, classifier='xgb')\n",
    "\n",
    "# print('XGBoost')\n",
    "# print(f\"Mean AUC: {round(np.mean(xgb_auc_class), 2)}, Median AUC: {round(np.median(xgb_auc_class), 2)}, Min AUC: {round(np.min(xgb_auc_class), 2)}, Max AUC: {round(np.max(xgb_auc_class), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train doc2vec vectors and train xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vector_size = [10, 20, 50, 100, 200]\n",
    "window_size = [5]\n",
    "epochs = [10]\n",
    "\n",
    "for v in vector_size:\n",
    "    for w in window_size:\n",
    "        for e in epochs:\n",
    "            dm_model = Doc2Vec(X_train_docs, vector_size=v, window=w, epochs=e, \n",
    "                            min_count=5, dm=1)\n",
    "            dbow_model = Doc2Vec(X_train_docs, vector_size=v, window=w, epochs=e, \n",
    "                            min_count=5, dm=0)\n",
    "            \n",
    "            X_train_dm_dv = []\n",
    "            X_train_dbow_dv = []\n",
    "            \n",
    "            X_test_dm_dv = []\n",
    "            X_test_dbow_dv = []\n",
    "            \n",
    "            for i in range(len(X_train_docs)):\n",
    "                X_train_dm_dv.append(dm_model.docvecs[i])\n",
    "                X_train_dbow_dv.append(dbow_model.docvecs[i])\n",
    "\n",
    "            for i in range(len(X_test_docs)):\n",
    "                X_test_dm_dv.append(dm_model.infer_vector(X_test_docs[i][0]))\n",
    "                X_test_dbow_dv.append(dbow_model.infer_vector(X_test_docs[i][0]))\n",
    "                \n",
    "            X_train_dv = pd.concat([pd.DataFrame(X_train_dm_dv),\n",
    "                                            pd.DataFrame(X_train_dbow_dv)], axis=1)\n",
    "            X_test_dv = pd.concat([pd.DataFrame(X_test_dm_dv),\n",
    "                                             pd.DataFrame(X_test_dbow_dv)], axis=1)\n",
    "\n",
    "#             _fit = RandomForestClassifier(n_estimators=50)\n",
    "            auc_class = train_eval_classifiers(X_train_dv, X_test_dv, y_train, y_test)\n",
    "            print(f'Vector size: {v}, Window size: {w}, Epoch: {e}')\n",
    "            print(f\"Mean AUC: {round(np.mean(auc_class), 2)}, Median AUC: {round(np.median(auc_class), 2)}, Min AUC: {round(np.min(auc_class), 2)}, Max AUC: {round(np.max(auc_class), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([[1,2,3]]), pd.DataFrame([[4,5,6]])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_model.docvecs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
